{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m7riBM3wOLFn"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "jAC0TpzkyBJD"
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H0nyItXbRO2q"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.cross_decomposition import CCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cross_decomposition import PLSRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "if8dH_rGakQw"
   },
   "outputs": [],
   "source": [
    "import dgl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchmetrics.classification import F1Score\n",
    "\n",
    "from dgl.nn.pytorch import GraphConv\n",
    "from ray import tune"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zMEsnj3fOOpT"
   },
   "source": [
    "# Starting with the gene data\n",
    "\n",
    "Building the graph and getting gene names, ids and if they are pathogenic or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w6Doob6i0XNQ",
    "outputId": "9ef87a0f-00a0-40f9-96cf-0ed196535528"
   },
   "outputs": [],
   "source": [
    "links = []\n",
    "with open(\"Data\\human_links.tsv\") as file: # the dataset containing genes and the genes they interact with\n",
    "    tsv_file = csv.reader(file, delimiter=\"\\t\")\n",
    "    for line in tsv_file:\n",
    "        links.append(line)\n",
    "\n",
    "links = np.asarray(links)\n",
    "links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "OuZO5jL7ZzHP",
    "outputId": "a14587fa-0639-4eb3-f561-629175356c1e"
   },
   "outputs": [],
   "source": [
    "links_df = pd.DataFrame(links, columns =['gene1', 'gene2'])\n",
    "links_df\n",
    "# read: there is an interaction between gene1 and gene2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O-XSsg_hyKYK"
   },
   "outputs": [],
   "source": [
    "G = nx.Graph()\n",
    "G.add_nodes_from([]) # number of nodes added = number of unique genes in links df that is added below.\n",
    "G.add_edges_from(np.asarray(links_df)) # add the connections between the nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0fumvqChWi3V",
    "outputId": "234a42a2-3fb7-45b7-8f02-997698cf8691"
   },
   "outputs": [],
   "source": [
    "nodes_ss = list(G.nodes) # all of the genes in the graph - will be used later to filter\n",
    "nodes_ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "s2dHprQQ36au",
    "outputId": "91f9f0f9-0298-4c8d-e867-6e4de528196e"
   },
   "outputs": [],
   "source": [
    "# the data that tells us if a gene is pathogenic or not\n",
    "\n",
    "gene_data_non_path = pd.read_excel(\"Data\\interactome_vs_mitocarta.xlsx\", sheet_name=\"non-pathogenic\", names=[\"gene\"])\n",
    "gene_data_non_path[\"pathogenic\"] = 0\n",
    "\n",
    "gene_data_path = pd.read_excel(\"Data\\interactome_vs_mitocarta.xlsx\", sheet_name=\"pathogenic_in_interactome\", names=[\"gene\"])\n",
    "gene_data_path[\"pathogenic\"] = 1\n",
    "\n",
    "# concat the pathogenic and non-pathogenic genes\n",
    "gene_data = pd.concat([gene_data_non_path, gene_data_path])\n",
    "\n",
    "# shuffle the genes so that the pathogenic and non-pathogenic genes are mixed\n",
    "gene_data = gene_data.sample(frac=1, random_state=207).reset_index().drop([\"index\"], axis=1)\n",
    "gene_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "jcDp2TyIX7AW",
    "outputId": "c62cf75a-e57f-4ebf-a612-765f7ec3f8d0"
   },
   "outputs": [],
   "source": [
    "# filter the genes and keep only the genes that are in the graph\n",
    "gene_data_ss = gene_data.loc[gene_data['gene'].isin(nodes_ss)]\n",
    "gene_data_ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "gOfVHX40Vy4O",
    "outputId": "4314cfbf-c591-43be-854f-229200eb36ff"
   },
   "outputs": [],
   "source": [
    "# get the HGNC symbols and ENSG id for all the genes to merge the datasets later\n",
    "gene_functions = pd.read_csv(\"Data\\human_gene_function.txt\", delimiter=\"\\t\")\n",
    "gene_functions = gene_functions[[\"HGNC_symbol\", \"ENSG_ID\"]]\n",
    "gene_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "hqh8QByAb-0w",
    "outputId": "1f27df29-8cd0-435a-b31f-ed25d921c97c"
   },
   "outputs": [],
   "source": [
    "# get the HGNC symbol for all the genes\n",
    "gene_data_ss = gene_data_ss.loc[gene_data_ss['gene'].isin(gene_functions[\"HGNC_symbol\"])]\n",
    "gene_data_ss = gene_data_ss.merge(gene_functions, left_on=\"gene\", right_on=\"HGNC_symbol\").drop(\"HGNC_symbol\", axis=1)\n",
    "gene_data_ss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bU2h_uFkOEE-"
   },
   "source": [
    "## Transcriptomics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 488
    },
    "id": "dW44EptFW9eh",
    "outputId": "63894b3e-f07f-4321-9d30-c6351f997ef5"
   },
   "outputs": [],
   "source": [
    "transcriptomics = pd.read_csv(\"Data\\smirnov_transcriptomics.csv\")\n",
    "transcriptomics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 488
    },
    "id": "56QF3utLXzUr",
    "outputId": "91daa929-9df0-49e7-9987-02c5972bc90b"
   },
   "outputs": [],
   "source": [
    "# some string comprehension to get the \"base\" ENSG id\n",
    "transcriptomics[\"gene_id\"] = transcriptomics[\"gene_id\"].str.split('\\.').str[0].str.strip()\n",
    "transcriptomics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 488
    },
    "id": "NhOESiB6QjA1",
    "outputId": "ad5a48f3-cdb8-424c-97f0-84399b73c234"
   },
   "outputs": [],
   "source": [
    "transcriptomics = transcriptomics.rename({'gene_id': 'geneID'}, axis=1)\n",
    "\n",
    "# I create a separate dataset specifically for CCA and PLS, since the structure is different\n",
    "# merge, drop duplicate columns, and rename columns\n",
    "all_data_tr_weights_analysis = pd.merge(gene_data_ss, transcriptomics, left_on=\"ENSG_ID\", right_on=\"geneID\", how=\"inner\")\n",
    "all_data_tr_weights_analysis = all_data_tr_weights_analysis.drop([\"ENSG_ID\", \"geneID\"], axis=1)\n",
    "all_data_tr_weights_analysis = all_data_tr_weights_analysis.rename({'gene': 'geneID'}, axis=1)\n",
    "all_data_tr_weights_analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LYaKqUd9PV-u"
   },
   "source": [
    "## Proteomics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 488
    },
    "id": "vgs4ErlTlLDe",
    "outputId": "cb598d21-f44b-4ef8-d282-2aab10b74b28"
   },
   "outputs": [],
   "source": [
    "proteomics = pd.read_csv(\"Data\\smirnov_proteomics.tsv\", delimiter=\"\\t\")\n",
    "proteomics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 488
    },
    "id": "Fhga8xNVL3RF",
    "outputId": "a0ea94db-310e-485e-de0e-993ce03db6df"
   },
   "outputs": [],
   "source": [
    "# keep the patients that are in both datasets\n",
    "all_data_pr_weights_analysis = proteomics[proteomics.columns.intersection(all_data_tr_weights_analysis.columns)]\n",
    "all_data_pr_weights_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 488
    },
    "id": "R2cZGbveOwM4",
    "outputId": "14dbb664-4b33-45ee-bfa4-5167fd234df7"
   },
   "outputs": [],
   "source": [
    "all_data_tr_weights_analysis = all_data_tr_weights_analysis[all_data_tr_weights_analysis.columns.intersection(proteomics.columns)]\n",
    "all_data_tr_weights_analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S4lzGrLyP20N"
   },
   "source": [
    "Back to the data for the graphs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 488
    },
    "id": "kn2hfwDF4YgH",
    "outputId": "bb60d287-b742-411a-a259-70f856fcbdbe"
   },
   "outputs": [],
   "source": [
    "all_data_tr = pd.merge(gene_data_ss, transcriptomics, left_on=\"ENSG_ID\", right_on=\"geneID\", how=\"inner\")\n",
    "all_data_tr = all_data_tr.drop([\"ENSG_ID\", \"geneID\"], axis=1)\n",
    "all_data_tr = all_data_tr.set_index([\"gene\", \"pathogenic\"]).add_prefix(\"tr__\").reset_index() # add prefix to identify transcriptomic\n",
    "all_data_tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 488
    },
    "id": "QK8LVw9d4k1b",
    "outputId": "c5538ffb-4762-45a0-ae4d-a523eb654806",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "all_data_pr = pd.merge(gene_data_ss, proteomics, left_on=\"gene\", right_on=\"geneID\", how=\"inner\")\n",
    "all_data_pr = all_data_pr.drop([\"ENSG_ID\", \"geneID\"], axis=1)\n",
    "all_data_pr = all_data_pr.set_index(['gene', 'pathogenic']).add_prefix('pr__').reset_index() # add prefix to identify proteomic\n",
    "all_data_pr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale and transform proteomics and transcriptomics separately before merging\n",
    "scaler = MinMaxScaler()\n",
    "transformer = QuantileTransformer(n_quantiles=100, output_distribution='normal')\n",
    "\n",
    "all_data_tr.iloc[:, 2:] = scaler.fit_transform(transformer.fit_transform(all_data_tr.iloc[:, 2:]))\n",
    "all_data_pr.iloc[:, 2:] = scaler.fit_transform(transformer.fit_transform(all_data_pr.iloc[:, 2:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dhvsi2gyQJIJ"
   },
   "source": [
    "## The final clean data\n",
    "With proteomics and transcriptomics merged together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 488
    },
    "id": "-f47_MuHXmGO",
    "outputId": "fc32a4f8-31d4-43d7-ca6f-f0dd83a9175b"
   },
   "outputs": [],
   "source": [
    "# We now have: Rows = Genes, Columns = each patient's transcriptomic or proteomic RNA-seq count per gene\n",
    "all_data = pd.merge(all_data_tr, all_data_pr, on=[\"gene\", \"pathogenic\"], how=\"inner\")\n",
    "all_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WEJJ2_MARYiO"
   },
   "source": [
    "# Check for duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-WTl0AbMX93J",
    "outputId": "ecf0c27c-f77a-4b89-c34c-ded2efb66293"
   },
   "outputs": [],
   "source": [
    "idx = all_data['gene'][all_data['gene'].duplicated(keep=False)]\n",
    "idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(idx.index)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 488
    },
    "id": "yrOy30iTXNLs",
    "outputId": "27ea9c97-a455-4505-ffbc-d048784486a1"
   },
   "outputs": [],
   "source": [
    "# remove the duplicated gene\n",
    "all_data = all_data.drop(list(idx.index)[1], axis=0)\n",
    "all_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lEHUVudjRhvB"
   },
   "source": [
    "Finally, there are 716 genes and values for 230 proteomic and 146 transcriptomic patients. To build the graph the difference doesn't matter since each gene gets stored in an array in each node - no need for transcriptomics = proteomics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BWySPtTPSeTz"
   },
   "source": [
    "# Creating the final graph with the 716 genes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 319
    },
    "id": "fFeVVUsvXdnb",
    "outputId": "6cc2264a-7147-40bc-c0b2-e4994d90696c"
   },
   "outputs": [],
   "source": [
    "# we create one graph for each \"weights analysis\": The none, CCA, PLS, and pearson analyses\n",
    "G_none = nx.subgraph(G, all_data[\"gene\"])\n",
    "G_cca = nx.subgraph(G, all_data[\"gene\"])\n",
    "G_pls = nx.subgraph(G, all_data[\"gene\"])\n",
    "G_pearson = nx.subgraph(G, all_data[\"gene\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I6l3V1tMXqPU"
   },
   "outputs": [],
   "source": [
    "# The nodes need to be labelled numerically for the DGL library, so we do this\n",
    "relabel_nodes = dict((v,k) for k,v in all_data.iloc[:,0].to_dict().items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_none = nx.relabel_nodes(G_none, relabel_nodes, copy=True)\n",
    "G_cca = nx.relabel_nodes(G_cca, relabel_nodes, copy=True)\n",
    "G_pls = nx.relabel_nodes(G_pls, relabel_nodes, copy=True)\n",
    "G_pearson = nx.relabel_nodes(G_pearson, relabel_nodes, copy=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VayXxFtQY31T"
   },
   "source": [
    "# Adding weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HKeXyg6KcIlg"
   },
   "source": [
    "## Canonical Correlation Analysis for weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 488
    },
    "id": "Ky-TiLUsQT4T",
    "outputId": "c262e639-f003-48c5-f0d0-b051b92d5919"
   },
   "outputs": [],
   "source": [
    "# we need to find the correlations between the genes per omic, so the genes need to become the columns -> transpose datasets\n",
    "all_data_tr_cca_transposed = all_data_tr_weights_analysis.T\n",
    "all_data_tr_cca_transposed.columns = all_data_tr_cca_transposed.iloc[0] #set row 0 as column names\n",
    "all_data_tr_cca_transposed = all_data_tr_cca_transposed.drop([\"geneID\"], axis=0) # remove row with pathogenic label\n",
    "all_data_tr_cca_transposed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 488
    },
    "id": "4oGmbYYcQwkf",
    "outputId": "390b087d-06dd-4ab5-ea32-203df00107b8"
   },
   "outputs": [],
   "source": [
    "all_data_pr_cca_transposed = all_data_pr_weights_analysis.T\n",
    "all_data_pr_cca_transposed.columns = all_data_pr_cca_transposed.iloc[0] #set row 0 as column names\n",
    "all_data_pr_cca_transposed = all_data_pr_cca_transposed.drop([\"geneID\"], axis=0) # remove row with pathogenic label\n",
    "all_data_pr_cca_transposed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rzsB94767_Qk"
   },
   "outputs": [],
   "source": [
    "# CCA\n",
    "X1 = all_data_pr_cca_transposed\n",
    "X2 = all_data_tr_cca_transposed\n",
    "\n",
    "# same scaling and transforming steps as previously\n",
    "scaler = MinMaxScaler()\n",
    "transformer = QuantileTransformer(n_quantiles=100, output_distribution='normal')\n",
    "\n",
    "X1_sc = scaler.fit_transform(transformer.fit_transform(X1))\n",
    "X2_sc = scaler.fit_transform(transformer.fit_transform(X2))\n",
    "\n",
    "n_comp=1 #choose number of canonical variates pairs \n",
    "cca = CCA(scale=False, n_components=n_comp) #define CCA\n",
    "cca.fit(X1_sc, X2_sc) #fit scaled data\n",
    "X1_c, X2_c = cca.transform(X1_sc, X2_sc) #transform our datasests to obtain canonical variates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 519
    },
    "id": "pvmdVwc47_Eu",
    "outputId": "fae14dd7-9a5d-41df-b18f-c72fa4907a64"
   },
   "outputs": [],
   "source": [
    "# get analysis results into correlation df\n",
    "coef_df = pd.DataFrame(np.round(cca.coef_, 6), columns = [X2.columns])\n",
    "coef_df.index = X1.columns\n",
    "coef_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 333
    },
    "id": "Lwp9S38X-2jA",
    "outputId": "1ffc80c6-3bf3-4bea-9b74-b55a966a1346"
   },
   "outputs": [],
   "source": [
    "# check the heatmap of the first 30 genes\n",
    "import seaborn as sns\n",
    "sns.heatmap(coef_df.iloc[0:30, 0:30], cmap='coolwarm', annot=False, linewidths=1, vmin=-0.032)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def melt_and_clean(coef_df):\n",
    "    # get coefficients to use as weights\n",
    "    coef_df_melted = pd.melt(coef_df, id_vars=None, value_vars=None, var_name='column', value_name='value', \n",
    "                             ignore_index = False).reset_index()\n",
    "    coef_df_melted = coef_df_melted.rename({\"geneID\":\"node1\", \"column\":\"node2\", \"value\":\"weight\"}, axis=1)\n",
    "\n",
    "    # remove duplicates\n",
    "    coef_df_melted = coef_df_melted.drop_duplicates(subset=['node1', 'node2'], keep='first')\n",
    "    return(coef_df_melted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_df_melted_cca = melt_and_clean(coef_df)\n",
    "coef_df_melted_cca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep only the genes that we have in our links df\n",
    "cca_graph_genes = pd.merge(links_df, coef_df_melted_cca, left_on=[\"gene1\", \"gene2\"], right_on=[\"node1\", \"node2\"], how=\"inner\")\n",
    "cca_graph_genes = cca_graph_genes.drop([\"gene1\", \"gene2\"], axis=1)\n",
    "cca_graph_genes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_graph_weights(graph_genes, neg_weights):\n",
    "    # keep only the genes that is that is in the final data\n",
    "    graph_genes_ = graph_genes[graph_genes['node1'].isin(all_data['gene'])]\n",
    "    graph_genes_ = graph_genes_[graph_genes_[\"node2\"].isin(all_data['gene'])]\n",
    "    if neg_weights == False:\n",
    "        graph_genes_[\"weight\"] = np.abs(graph_genes_[\"weight\"])\n",
    "    return(graph_genes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_node_names_to_numeric(graph_genes_):\n",
    "    # map node names to numerical node numbers for DGL\n",
    "    graph_genes_['n1'] = graph_genes_['node1'].map(lambda x: all_data.set_index('gene').index.get_loc(x))\n",
    "    graph_genes_['n2'] = graph_genes_['node2'].map(lambda x: all_data.set_index('gene').index.get_loc(x))\n",
    "    return(graph_genes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_valid_edges(graph_genes, G_, neg_weights):\n",
    "    \n",
    "    graph_genes_ = get_graph_weights(graph_genes, neg_weights)\n",
    "    graph_genes_ = map_node_names_to_numeric(graph_genes_) # takes a while\n",
    "    #display(graph_genes_)\n",
    "    \n",
    "    # create a dict with a tuple of the interacting genes as the key and the weight as value - this is the format needed to add\n",
    "    # it to the graph\n",
    "    genes_dict = graph_genes_.iloc[:,2:].set_index([\"n1\", \"n2\"]).to_dict(orient='index')\n",
    "    \n",
    "    nx.set_edge_attributes(G_, genes_dict, \"name\")\n",
    "    # only the edges that exist in the graph now have a weight, so we need to extract these \"valid\" edges\n",
    "    \n",
    "    valid_edges = nx.get_edge_attributes(G_, \"name\")\n",
    "    #print(valid_edges)\n",
    "    # create df of valid edges\n",
    "    valid_analysis_edges = pd.DataFrame.from_dict(valid_edges).T.reset_index()\n",
    "    valid_analysis_edges = valid_analysis_edges.rename({'level_0': 'node1', \"level_1\":\"node2\"}, axis=1)\n",
    "    return(valid_analysis_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_cca_edges = get_valid_edges(cca_graph_genes, G_cca, neg_weights=True)\n",
    "valid_cca_edges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZA3wWtnZfc6o"
   },
   "source": [
    "# PLS Regression for determining weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TXLdsVsB-2fN"
   },
   "outputs": [],
   "source": [
    "n_comp=1 \n",
    "PLSR = PLSRegression(scale=False, n_components=n_comp) \n",
    "PLSR.fit(X1_sc, X2_sc) \n",
    "X1_c, X2_c = PLSR.transform(X1_sc, X2_sc) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 519
    },
    "id": "GzDLP9dcWS40",
    "outputId": "f24d3119-4dd3-4f84-89b9-25fded57c1c4"
   },
   "outputs": [],
   "source": [
    "# get analysis results into correlation df\n",
    "coef_df_pls = pd.DataFrame(np.round(PLSR.coef_, 6), columns = [X2.columns])\n",
    "coef_df_pls.index = X1.columns\n",
    "coef_df_pls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 333
    },
    "id": "6W8zcXkWWS1K",
    "outputId": "efe5fee5-403f-4e8b-e5da-48a7fe50d47f"
   },
   "outputs": [],
   "source": [
    "# check the heatmap of the first 30 genes\n",
    "import seaborn as sns\n",
    "sns.heatmap(coef_df_pls.iloc[0:30, 0:30], cmap='coolwarm', annot=False, linewidths=1, vmin=-0.00025)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_df_melted_pls = melt_and_clean(coef_df_pls)\n",
    "coef_df_melted_pls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep only the genes in the graph\n",
    "pls_graph_genes = pd.merge(links_df, coef_df_melted_pls, left_on=[\"gene1\", \"gene2\"], right_on=[\"node1\", \"node2\"], how=\"inner\")\n",
    "pls_graph_genes = pls_graph_genes.drop([\"gene1\", \"gene2\"], axis=1)\n",
    "pls_graph_genes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_pls_edges = get_valid_edges(pls_graph_genes, G_pls, neg_weights=True)\n",
    "valid_pls_edges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pearson's cross correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need the same column names for transcriptomics and proteomics\n",
    "pearson_keep_columns = list(set(all_data_pr_cca_transposed.columns) & set(all_data_tr_cca_transposed.columns))\n",
    "all_data_pr_pc_transposed = all_data_pr_cca_transposed[pearson_keep_columns]\n",
    "all_data_tr_pc_transposed = all_data_tr_cca_transposed[pearson_keep_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove duplicated column\n",
    "all_data_tr_pc_transposed = all_data_tr_pc_transposed.loc[:,~all_data_tr_pc_transposed.columns.duplicated()].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "corr_matrix = pd.DataFrame(np.corrcoef(all_data_pr_pc_transposed.T.values.astype(float), \n",
    "                                       all_data_tr_pc_transposed.T.values.astype(float)))\n",
    "corr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To get the cross correlation we need to take either the upper right or lower left of the matrix\n",
    "# --                                     --\n",
    "# | correlation         cross correlation |\n",
    "# | cross correlation   correlation       |\n",
    "# --                                     --\n",
    "\n",
    "# taking upper right\n",
    "corr_matrix = corr_matrix.iloc[:716, 716:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# rename columns and rows to genes\n",
    "corr_matrix = corr_matrix.rename(columns={old: new for old, new in zip(corr_matrix.columns, all_data_pr_pc_transposed.columns)})\n",
    "corr_matrix.index = all_data_pr_pc_transposed.columns\n",
    "corr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_df_melted_pearson = melt_and_clean(corr_matrix)\n",
    "coef_df_melted_pearson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep only the genes in the graph\n",
    "pearson_graph_genes = pd.merge(links_df, coef_df_melted_pearson, left_on=[\"gene1\", \"gene2\"], right_on=[\"node1\", \"node2\"], \n",
    "                               how=\"inner\")\n",
    "pearson_graph_genes = pearson_graph_genes.drop([\"gene1\", \"gene2\"], axis=1)\n",
    "pearson_graph_genes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_pearson_edges = get_valid_edges(pearson_graph_genes, G_pearson, neg_weights=True)\n",
    "valid_pearson_edges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_evpX-0doaDt"
   },
   "source": [
    "# Building the DGL graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hhNG7rFfo4V3"
   },
   "outputs": [],
   "source": [
    "def build_dgl_network(graph_weights_method, edge_df, weight_method):\n",
    "    G_dgl = dgl.from_networkx(graph_weights_method) # transform to dgl graph\n",
    "    G_dgl.ndata['feat'] = torch.from_numpy(all_data.iloc[:, 2:].values) # add omics data to nodes\n",
    "  \n",
    "    if weight_method != \"None\":\n",
    "        # Create edge weights\n",
    "        edge_weights = np.array(edge_df[\"weight\"])\n",
    "      \n",
    "        # Add edges with weights\n",
    "        src = np.array(edge_df[\"node1\"])\n",
    "        dst = np.array(edge_df[\"node2\"])\n",
    "        edge_weights = torch.tensor(edge_weights)\n",
    "      \n",
    "        G_dgl.add_edges(torch.tensor(src), torch.tensor(dst), data={'weight': edge_weights})\n",
    "  \n",
    "    return(G_dgl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cFWRyJtO6pUg",
    "outputId": "abc855fe-b3dc-4d9e-dcbd-393851248616"
   },
   "outputs": [],
   "source": [
    "G_dgl_none = build_dgl_network(G_none, valid_pearson_edges, \"None\") # use any edge data as placeholder - will not be added\n",
    "G_dgl_none"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-PDSqYq4a6U-",
    "outputId": "3d596ee5-2b62-4d69-dd2e-01abc58e2001"
   },
   "outputs": [],
   "source": [
    "# can uncomment below and change neg_weights to change graph weights\n",
    "#valid_cca_edges = get_valid_edges(cca_graph_genes, G_cca, neg_weights=True)\n",
    "G_dgl_cca = build_dgl_network(G_cca, valid_cca_edges, \"CCA\")\n",
    "G_dgl_cca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tL_FfQ6p6pOt",
    "outputId": "d9b10a6d-4183-4e81-9a16-fb81115e77c3"
   },
   "outputs": [],
   "source": [
    "# can uncomment below and change neg_weights to change graph weights\n",
    "#valid_pls_edges = get_valid_edges(pls_graph_genes, G_pls, neg_weights=True)\n",
    "G_dgl_pls = build_dgl_network(G_pls, valid_pls_edges, \"PLS\")\n",
    "G_dgl_pls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# can uncomment below and change neg_weights to change graph weights\n",
    "#valid_pearson_edges = get_valid_edges(pearson_graph_genes, G_pearson, neg_weights=True)\n",
    "G_dgl_pearson = build_dgl_network(G_pearson, valid_pearson_edges, \"PEARSON\")\n",
    "G_dgl_pearson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wor5DMdUrGUG"
   },
   "source": [
    "# Building the GCN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FyNB_kKNdxcr"
   },
   "outputs": [],
   "source": [
    "class GCN(nn.Module):\n",
    "    def __init__(self, in_feats, hidden_size, num_classes, weight = torch.double):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GraphConv(in_feats, hidden_size, weight=weight)\n",
    "        self.conv2 = GraphConv(hidden_size, hidden_size, weight=weight)\n",
    "        self.conv3 = GraphConv(hidden_size, hidden_size, weight=weight)\n",
    "        self.conv4 = GraphConv(hidden_size, num_classes, weight=weight)\n",
    "\n",
    "    def forward(self, g, inputs):\n",
    "        h = self.conv1(g, inputs)\n",
    "        h = torch.relu(h)\n",
    "        h = self.conv2(g, h)\n",
    "        h = torch.relu(h)\n",
    "        h = self.conv3(g, h)\n",
    "        h = torch.relu(h)\n",
    "        h = self.conv4(g, h)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labeled_nodes(node_labels): \n",
    "    # Count the number of 0s and 1s\n",
    "    count_0 = np.count_nonzero(node_labels == 0)\n",
    "    count_1 = np.count_nonzero(node_labels == 1)\n",
    "    \n",
    "    num_to_select = 16\n",
    "    \n",
    "    # Create a list of indices of 0s and 1s\n",
    "    indices_0 = np.argwhere(node_labels == 0).flatten()\n",
    "    indices_1 = np.argwhere(node_labels == 1).flatten()\n",
    "    \n",
    "    # Randomly select indices\n",
    "    random.seed(207)\n",
    "    selected_indices_0 = random.sample(list(indices_0), num_to_select)\n",
    "    random.seed(702)\n",
    "    selected_indices_1 = random.sample(list(indices_1), num_to_select)\n",
    "    \n",
    "    final_indices = np.concatenate((selected_indices_0, selected_indices_1))\n",
    "    \n",
    "    # get unlabelled indices\n",
    "    unselected_indices = np.setdiff1d(np.arange(len(node_labels)), final_indices)\n",
    "    \n",
    "    return(final_indices, unselected_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WaFLsG5bea-X",
    "outputId": "5066ec30-2fba-42b7-f3e7-1035eb968033"
   },
   "outputs": [],
   "source": [
    "node_labels = all_data['pathogenic'].astype('category').cat.codes.to_numpy()\n",
    "random_labels, random_unlabelled = get_labeled_nodes(node_labels)\n",
    "node_labels = torch.from_numpy(node_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "weight_methods = [\"No weights\", \"CCA\", \"PLS\", \"Pearson\"]\n",
    "i = 0\n",
    "input_shape = all_data.iloc[:, 2:].shape[1]\n",
    "hidden_size = 50\n",
    "learning_rate = 0.0001\n",
    "\n",
    "for g in [G_dgl_none, G_dgl_cca, G_dgl_pls, G_dgl_pearson]:\n",
    "  \n",
    "    print(\"Graph:\", weight_methods[i])\n",
    "  \n",
    "    # build network\n",
    "    net = GCN(input_shape, hidden_size, 2, weight=torch.double)\n",
    "  \n",
    "    inputs = g.ndata['feat'].double()\n",
    "    labeled_nodes = torch.tensor(random_labels).long()\n",
    "    g.ndata['label'] = node_labels\n",
    "    labels = g.ndata['label']\n",
    "    unlabeled_nodes = torch.tensor(random_unlabelled).long()\n",
    "  \n",
    "    g = dgl.add_self_loop(g)\n",
    "  \n",
    "    # save all evaluation metrics\n",
    "    all_logits = []\n",
    "    labeled_loss = []\n",
    "    unlabeled_loss = []\n",
    "    labeled_acc = []\n",
    "    unlabeled_acc = []\n",
    "    labeled_f1 = []\n",
    "    unlabeled_f1 = []\n",
    "    \n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "    f1 = F1Score(task=\"multiclass\", num_classes=2, average=\"weighted\")\n",
    "    n_epochs = 1000\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        logits = net(g, inputs.float())\n",
    "        # save the logits for visualization later\n",
    "        all_logits.append(logits.detach())\n",
    "        logp = F.log_softmax(logits, 1)\n",
    "        \n",
    "        # labeled loss\n",
    "        loss = F.nll_loss(logp[labeled_nodes], labels[labeled_nodes].type(torch.LongTensor))\n",
    "        labeled_loss.append(loss)\n",
    "    \n",
    "        # labeled accuracy\n",
    "        _, predicted = torch.max(logp[labeled_nodes], 1)\n",
    "        correct = (predicted == labels[labeled_nodes]).sum().item()\n",
    "        accuracy = correct / len(labeled_nodes)\n",
    "        labeled_acc.append(accuracy)\n",
    "        \n",
    "        # labeled f1 score\n",
    "        f1_score = f1(predicted, labels[labeled_nodes])\n",
    "        labeled_f1.append(f1_score)\n",
    "        \n",
    "        # unlabeled loss\n",
    "        loss_unlabeled = F.nll_loss(logp[unlabeled_nodes], labels[unlabeled_nodes].type(torch.LongTensor))\n",
    "        unlabeled_loss.append(loss_unlabeled)\n",
    "    \n",
    "        # unlabeled accuracy\n",
    "        _, predicted = torch.max(logp[unlabeled_nodes], 1)\n",
    "        correct = (predicted == labels[unlabeled_nodes]).sum().item()\n",
    "        accuracy_unl = correct / len(unlabeled_nodes)\n",
    "        unlabeled_acc.append(accuracy_unl)\n",
    "        \n",
    "        # unlabeled f1 score\n",
    "        f1_score_unl = f1(predicted, labels[unlabeled_nodes])\n",
    "        unlabeled_f1.append(f1_score_unl)\n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "        if epoch % 10 == 0:\n",
    "            print('Epoch %d | Loss: %.4f  |  Loss Test: %.4f | Acc: %.4f  |  Acc Test: %.4f' % (epoch, loss, loss_unlabeled, accuracy, accuracy_unl))\n",
    "  \n",
    "    plt.plot(range(0, n_epochs), [loss.detach().numpy() for loss in labeled_loss], label=\"labeled\")\n",
    "    plt.plot(range(0, n_epochs), [loss.detach().numpy() for loss in unlabeled_loss], label=\"unlabeled\")\n",
    "    plt.legend()\n",
    "    plt.title(f\"Loss: {weight_methods[i]}\")\n",
    "    plt.show()\n",
    "  \n",
    "    plt.plot(range(0, n_epochs), [acc for acc in labeled_acc], label=\"labeled\")\n",
    "    plt.plot(range(0, n_epochs), [acc for acc in unlabeled_acc], label=\"unlabeled\")\n",
    "    plt.legend()\n",
    "    plt.title(f\"Accuracy: {weight_methods[i]}\")\n",
    "    plt.show()\n",
    "    \n",
    "    min_unl_loss = np.argmin([i.item() for i in unlabeled_loss])\n",
    "    print(f\"Lowest unlabelled loss: {min(unlabeled_loss).detach().item()} at epoch {min_unl_loss}, train loss: {[i.item() for i in labeled_loss][min_unl_loss]}\")\n",
    "    \n",
    "    print(f\"Maximum acc: {max(unlabeled_acc)} at epoch {min_unl_loss}, train acc: {labeled_acc[min_unl_loss]}\")\n",
    "\n",
    "    print(f\"Maximum f1: {max(unlabeled_f1)} at epoch {min_unl_loss}, train f1: {labeled_f1[min_unl_loss]}\")\n",
    "  \n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper parameter tuning with Ray Tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_eval_gcn(config, reporter):\n",
    "    input_shape = all_data.iloc[:, 2:].shape[1]\n",
    "    \n",
    "    g = config[\"g\"]\n",
    "    net = GCN(input_shape, config[\"hidden_neurons\"], 2, weight=torch.double)\n",
    "    \n",
    "    inputs = g.ndata['feat'].double()\n",
    "    labeled_nodes = torch.tensor(random_labels).long()\n",
    "    g.ndata['label'] = node_labels\n",
    "    labels = g.ndata['label']\n",
    "    unlabeled_nodes = torch.tensor(random_unlabelled).long()\n",
    "    \n",
    "    g = dgl.add_self_loop(g)\n",
    "    \n",
    "    # save all evaluation metrics\n",
    "    all_logits = []\n",
    "    labeled_loss = []\n",
    "    unlabeled_loss = []\n",
    "    labeled_acc = []\n",
    "    unlabeled_acc = []\n",
    "    labeled_f1 = []\n",
    "    unlabeled_f1 = []\n",
    "    \n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=config[\"learn_rate\"])\n",
    "    f1 = F1Score(task=\"multiclass\", num_classes=2, average=\"weighted\")\n",
    "    n_epochs = 1000\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        logits = net(g, inputs.float())\n",
    "        # save the logits for visualization later\n",
    "        all_logits.append(logits.detach())\n",
    "        logp = F.log_softmax(logits, 1)\n",
    "        \n",
    "        # labeled loss\n",
    "        loss = F.nll_loss(logp[labeled_nodes], labels[labeled_nodes].type(torch.LongTensor))\n",
    "        labeled_loss.append(loss)\n",
    "    \n",
    "        # labeled accuracy\n",
    "        _, predicted = torch.max(logp[labeled_nodes], 1)\n",
    "        correct = (predicted == labels[labeled_nodes]).sum().item()\n",
    "        accuracy = correct / len(labeled_nodes)\n",
    "        labeled_acc.append(accuracy)\n",
    "        \n",
    "        # labeled f1 score\n",
    "        f1_score = f1(predicted, labels[labeled_nodes])\n",
    "        labeled_f1.append(f1_score)\n",
    "        \n",
    "        # unlabeled loss\n",
    "        loss_unlabeled = F.nll_loss(logp[unlabeled_nodes], labels[unlabeled_nodes].type(torch.LongTensor))\n",
    "        unlabeled_loss.append(loss_unlabeled)\n",
    "    \n",
    "        # unlabeled accuracy\n",
    "        _, predicted = torch.max(logp[unlabeled_nodes], 1)\n",
    "        correct = (predicted == labels[unlabeled_nodes]).sum().item()\n",
    "        accuracy_unl = correct / len(unlabeled_nodes)\n",
    "        unlabeled_acc.append(accuracy_unl)\n",
    "        \n",
    "        # unlabeled f1 score\n",
    "        f1_score_unl = f1(predicted, labels[unlabeled_nodes])\n",
    "        unlabeled_f1.append(f1_score_unl)\n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    reporter({\"test loss\": min(unlabeled_loss).detach().item(), \"train loss\": [i.item() for i in labeled_loss][min_unl_loss], \n",
    "              \"test acc\": max(unlabeled_acc), \"train acc\": labeled_acc[min_unl_loss], \n",
    "              \"test f1score\": max(unlabeled_f1).item(), \"train f1score\": labeled_f1[min_unl_loss].item(), \"epoch\":min_unl_loss})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# runs very long\n",
    "weight_methods = [\"No weights\", \"CCA\", \"PLS\", \"Pearson\"]\n",
    "i = 0\n",
    "\n",
    "for g in [G_dgl_none, G_dgl_cca, G_dgl_pls, G_dgl_pearson]:   \n",
    "    \n",
    "    search_space = {\n",
    "    \"hidden_neurons\": tune.grid_search([20, 30, 50, 80, 100, 120]),\n",
    "    \"learn_rate\": tune.grid_search([0.01, 0.001, 0.0001, 0.00001]),\n",
    "    \"g\": g,}\n",
    "    \n",
    "    print(\"Graph:\", weight_methods[i])\n",
    "    tuner = tune.Tuner(train_eval_gcn, param_space=search_space)\n",
    "    results = tuner.fit()\n",
    "    i += 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create an animation of the GCN's training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "weight_methods = [\"CCA\"]\n",
    "i = 0\n",
    "input_shape = all_data.iloc[:, 2:].shape[1]\n",
    "hidden_size = 100\n",
    "learning_rate = 0.0001\n",
    "\n",
    "for g in [G_dgl_cca]:\n",
    "  \n",
    "    print(\"Graph:\", weight_methods[i])\n",
    "  \n",
    "    # build network\n",
    "    net = GCN(input_shape, hidden_size, 2, weight=torch.double)\n",
    "  \n",
    "    inputs = g.ndata['feat'].double()\n",
    "    labeled_nodes = torch.tensor(random_labels).long()\n",
    "    g.ndata['label'] = node_labels\n",
    "    labels = g.ndata['label']\n",
    "    unlabeled_nodes = torch.tensor(random_unlabelled).long()\n",
    "  \n",
    "    g = dgl.add_self_loop(g)\n",
    "  \n",
    "    # save all evaluation metrics\n",
    "    all_logits = []\n",
    "    labeled_loss = []\n",
    "    unlabeled_loss = []\n",
    "    labeled_acc = []\n",
    "    unlabeled_acc = []\n",
    "    labeled_f1 = []\n",
    "    unlabeled_f1 = []\n",
    "    \n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "    f1 = F1Score(task=\"multiclass\", num_classes=2, average=\"weighted\")\n",
    "    n_epochs = 1000\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        logits = net(g, inputs.float())\n",
    "        # save the logits for visualization later\n",
    "        all_logits.append(logits.detach())\n",
    "        logp = F.log_softmax(logits, 1)\n",
    "        \n",
    "        # labeled loss\n",
    "        loss = F.nll_loss(logp[labeled_nodes], labels[labeled_nodes].type(torch.LongTensor))\n",
    "        labeled_loss.append(loss)\n",
    "    \n",
    "        # labeled accuracy\n",
    "        _, predicted = torch.max(logp[labeled_nodes], 1)\n",
    "        correct = (predicted == labels[labeled_nodes]).sum().item()\n",
    "        accuracy = correct / len(labeled_nodes)\n",
    "        labeled_acc.append(accuracy)\n",
    "        \n",
    "        # labeled f1 score\n",
    "        f1_score = f1(predicted, labels[labeled_nodes])\n",
    "        labeled_f1.append(f1_score)\n",
    "        \n",
    "        # unlabeled loss\n",
    "        loss_unlabeled = F.nll_loss(logp[unlabeled_nodes], labels[unlabeled_nodes].type(torch.LongTensor))\n",
    "        unlabeled_loss.append(loss_unlabeled)\n",
    "    \n",
    "        # unlabeled accuracy\n",
    "        _, predicted = torch.max(logp[unlabeled_nodes], 1)\n",
    "        correct = (predicted == labels[unlabeled_nodes]).sum().item()\n",
    "        accuracy_unl = correct / len(unlabeled_nodes)\n",
    "        unlabeled_acc.append(accuracy_unl)\n",
    "        \n",
    "        # unlabeled f1 score\n",
    "        f1_score_unl = f1(predicted, labels[unlabeled_nodes])\n",
    "        unlabeled_f1.append(f1_score_unl)\n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "        if epoch % 10 == 0:\n",
    "            print('Epoch %d | Loss: %.4f  |  Loss Test: %.4f | Acc: %.4f  |  Acc Test: %.4f' % (epoch, loss, loss_unlabeled, accuracy, accuracy_unl))\n",
    "  \n",
    "    plt.plot(range(0, n_epochs), [loss.detach().numpy() for loss in labeled_loss], label=\"labeled\")\n",
    "    plt.plot(range(0, n_epochs), [loss.detach().numpy() for loss in unlabeled_loss], label=\"unlabeled\")\n",
    "    plt.legend()\n",
    "    plt.title(f\"Loss: {weight_methods[i]}\")\n",
    "    plt.show()\n",
    "  \n",
    "    plt.plot(range(0, n_epochs), [acc for acc in labeled_acc], label=\"labeled\")\n",
    "    plt.plot(range(0, n_epochs), [acc for acc in unlabeled_acc], label=\"unlabeled\")\n",
    "    plt.legend()\n",
    "    plt.title(f\"Accuracy: {weight_methods[i]}\")\n",
    "    plt.show()\n",
    "    \n",
    "    min_unl_loss = np.argmin([i.item() for i in unlabeled_loss])\n",
    "    print(f\"Lowest unlabelled loss: {min(unlabeled_loss).detach().item()} at epoch {min_unl_loss}, train loss: {[i.item() for i in labeled_loss][min_unl_loss]}\")\n",
    "    \n",
    "    print(f\"Maximum acc: {max(unlabeled_acc)} at epoch {min_unl_loss}, train acc: {labeled_acc[min_unl_loss]}\")\n",
    "\n",
    "    print(f\"Maximum f1: {max(unlabeled_f1)} at epoch {min_unl_loss}, train f1: {labeled_f1[min_unl_loss]}\")\n",
    "  \n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jeACgczcfnCZ"
   },
   "outputs": [],
   "source": [
    "import matplotlib.animation as animation\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# make sure to run the train the GCN with preferred hyper-parameters and weights first\n",
    "\n",
    "def draw(i):\n",
    "    cls1color = '#00FFFF'\n",
    "    cls2color = '#FF00FF'\n",
    "    pos = {}\n",
    "    colors = []\n",
    "    for v in range(716):\n",
    "        pos[v] = all_logits[i][v].numpy()\n",
    "        cls = pos[v].argmax()\n",
    "        colors.append(cls1color if cls else cls2color)\n",
    "    ax.cla()\n",
    "    ax.axis('off')\n",
    "    ax.set_title('Epoch: %d' % i)\n",
    "    nx.draw_networkx(G_cca.to_undirected(), node_color=colors,\n",
    "            with_labels=False, node_size=5, ax=ax)\n",
    "\n",
    "fig = plt.figure(dpi=150)\n",
    "fig.clf()\n",
    "ax = fig.subplots()\n",
    "draw(0)  # draw the prediction of the first epoch\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_test = G_cca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove = [node for node,degree in dict(G_test.degree()).items() if degree <= 2] # remove nodes with few edges for simplicity\n",
    "G_test.remove_nodes_from(remove)\n",
    "\n",
    "def draw(i):\n",
    "    cls1color = '#C13002'\n",
    "    cls2color = '#50006C'\n",
    "    pos = {}\n",
    "    colors = []\n",
    "    for v in range(len(G_test.nodes)):\n",
    "        pos[v] = all_logits[i][v].numpy()\n",
    "        cls = pos[v].argmax()\n",
    "        colors.append(cls1color if cls else cls2color)\n",
    "    ax.cla()\n",
    "    ax.axis('off')\n",
    "    ax.set_title('Epoch: %d' % i)\n",
    "    p=nx.spring_layout(G_test)\n",
    "    nx.draw_networkx(G_test.to_undirected(), p, node_color=colors,\n",
    "            with_labels=False, node_size=5, ax=ax, width=0.1)\n",
    "    \n",
    "fig = plt.figure(dpi=150)\n",
    "fig.clf()\n",
    "ax = fig.subplots()\n",
    "draw(0)  # draw the prediction of the first epoch\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m-EXwXxXfm_h"
   },
   "outputs": [],
   "source": [
    "from matplotlib import rc\n",
    "import matplotlib\n",
    "\n",
    "matplotlib.rcParams['animation.embed_limit'] = 2**128\n",
    "rc('animation', html='jshtml')\n",
    "ani = animation.FuncAnimation(fig, draw, frames=len(all_logits), interval=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 345
    },
    "id": "8a9PC6RCfm8x",
    "outputId": "b6cd9c53-a057-40c8-b204-e1cf43d11307",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# runs for a very long time\n",
    "ani"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
